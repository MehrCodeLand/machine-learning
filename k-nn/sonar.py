# -*- coding: utf-8 -*-
"""advanced_sonar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e4SBinXlkpT1uyhBZRMGUfPFUmUsUhPU
"""

import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler , LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report , confusion_matrix , accuracy_score

import matplotlib.pyplot as plt

df = pd.read_csv('/content/sonar.all-data.csv')

label_encoder = LabelEncoder()
df['Target'] = label_encoder.fit_transform(df['Label'])

df.head()

# Visualize correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.drop('Label', axis=1).corr(), cmap='coolwarm', annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

X = df.drop(['Label' , 'Target'] , axis = 1 )
y = df['Target']

X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.1 , stratify=y , random_state=42)

#pipeline
pipeline = Pipeline([
    ('scaler' , StandardScaler()),
    ('knn' , KNeighborsClassifier())
])

param_grid = {
    'knn__n_neighbors': list(range(1,31)),
    'knn__metric': ['euclidean' , 'manhattan' , 'minkowski']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train , y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

y_pred = grid_search.best_estimator_.predict(X_test)

# Print accuracy and classification report
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# using model
new_sample = np.array([[0.02, 0.0371, 0.0428, 0.0207, 0.0954, 0.0986, 0.1539, 0.1601, 0.3109, 0.2111,
                        0.2872, 0.3071, 0.5281, 0.5071, 0.4791, 0.5631, 0.6291, 0.6731, 0.7351, 0.7201,
                        0.6971, 0.6521, 0.5531, 0.4771, 0.4011, 0.3551, 0.3171, 0.2711, 0.2511, 0.2011,
                        0.1511, 0.1291, 0.1011, 0.0911, 0.0811, 0.0711, 0.0651, 0.0591, 0.0531, 0.0491,
                        0.0411, 0.0351, 0.0311, 0.0271, 0.0231, 0.0211, 0.0191, 0.0171, 0.0151, 0.0131,
                        0.0111, 0.0091, 0.0071, 0.0051, 0.0031, 0.0021, 0.0011, 0.0001, 0.0001, 0.0001]])

# Standardize the new data
new_sample_scaled = grid_search.best_estimator_.named_steps['scaler'].transform(new_sample)

# Predict the label
predicted_label = grid_search.best_estimator_.named_steps['knn'].predict(new_sample_scaled)

# Convert numerical prediction back to 'R' or 'M'
predicted_label = label_encoder.inverse_transform(predicted_label)

print("Predicted label:", predicted_label[0])

